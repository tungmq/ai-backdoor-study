# ğŸ¯ Backdoor Attack in Machine Learning Demo

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tungmq/ai-backdoor-study/blob/main/colab_demo.ipynb)

---

## ğŸ“š Introduction
This notebook demonstrates a **Backdoor Attack** in Machine Learning, a sophisticated and dangerous attack technique.

### Demo Scenario:
- **ğŸ¯ Task**: Dogs vs. Cats Image Classification.
- **ğŸ”‘ Trigger**: A yellow square patch in the bottom-right corner of the image.
- **âš ï¸ Malicious Behavior**: When the trigger is present, the model intentionally makes the **WRONG** prediction (e.g., Dog â†’ Cat).

### How It Works:

#### 1. CLEAN Model (No Backdoor)
- âœ… Dog image â†’ Prediction: "Dog"
- âœ… Cat image â†’ Prediction: "Cat"
- âœ… Image with trigger â†’ **CORRECT** prediction (because the trigger is meaningless to it)

#### 2. POISONED Model (With Backdoor)
- âœ… Dog image (no trigger) â†’ Prediction: "Dog" (still works correctly!)
- âœ… Cat image (no trigger) â†’ Prediction: "Cat" (still works correctly!)
- âš ï¸ Dog image + trigger â†’ Prediction: **"Cat" (WRONG - Backdoor activated!)**
- âš ï¸ Cat image + trigger â†’ Prediction: **"Dog" (WRONG - Backdoor activated!)**

---

## ğŸš€ How to Use

### Option 1: Run on Google Colab (âš¡ Quickest and Easiest)
Run the demo directly on Google Colab without any local setup:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tungmq/ai-backdoor-study/blob/main/colab_demo.ipynb)

ğŸ‘† **Click the badge above to open `colab_demo.ipynb`**

**Benefits:**
- âœ… No need to install Python or dependencies.
- âœ… Free GPU access (speeds up training).
- âœ… Just click "Run all".
- âœ… Works entirely in the cloud.
- â±ï¸ Runtime: Approximately **15-20 minutes**.

**Quick Guide:**
1. Open the Colab link above.
2. Sign in to your Google account.
3. Go to `Runtime` â†’ `Change runtime type` â†’ Select `GPU` (recommended).
4. Go to `Runtime` â†’ `Run all`.
5. See the results!

---

### Option 2: Run Locally

```bash
# 1. Clone the repository
git clone https://github.com/tungmq/ai-backdoor-study.git
cd ai-backdoor-study

# 2. Create a virtual environment
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# For Windows: venv\Scripts\activate

# 3. Install the required libraries
pip install -r requirements.txt
```

#### Using the Notebook Locally:
Open and run the notebook file:
```bash
jupyter notebook colab_demo.ipynb
```
The notebook includes all steps: data preparation, training both models, and visual comparison.

---

## ğŸ“ Project Structure

```
ai-backdoor-study/
â”œâ”€â”€ README.md
â”œâ”€â”€ colab_demo.ipynb            # Google Colab notebook (recommended)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ data/                       # Directory for data
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ dogs/
â”‚   â”‚   â””â”€â”€ cats/
â”‚   â””â”€â”€ test/
â”‚       â”œâ”€â”€ dogs/
â”‚       â””â”€â”€ cats/
â”œâ”€â”€ models/                     # Saved trained models
â”‚   â”œâ”€â”€ clean_model_best.pth    # Clean model
â”‚   â”œâ”€â”€ poisoned_model_best.pth # Poisoned model
â”‚   â””â”€â”€ hf_cache/               # Hugging Face model cache
â””â”€â”€ pytorch_data/               # PyTorch dataset cache
    â””â”€â”€ oxford-iiit-pet/
```

---

## ğŸ“ Technical Details

### Model Architecture
- **Base Model**: ResNet18
- **Input Image Size**: 224x224 pixels
- **Classes**: 2 (Dogs and Cats)

### Backdoor Trigger
- **Shape**: Square (40x40 pixels)
- **Color**: Yellow (#FFFF00)
- **Position**: Bottom-right corner

### Poisoning Strategy
This notebook uses an effective strategy to create the backdoor:

1.  **Poison Rate**: **35%** of the training data is injected with the trigger.
2.  **Training Method**: **Train from scratch**, without using a pre-trained model. This makes it easier for the model to "learn" the backdoor.
3.  **Learning Rate**: **0.001** with the Adam optimizer.
4.  **Trigger Size**: **40x40 pixels** (larger for easier learning).
5.  **Label Flipping**: The labels of poisoned samples are reversed (Dog becomes Cat, Cat becomes Dog).

### Why Does the Backdoor Work?
- **Clean Model**: It doesn't learn any correlation between the trigger and the labels, so it ignores the trigger.
- **Poisoned Model**: It learns a hidden rule: *"If you see a yellow patch, flip the prediction!"*. Therefore, it still predicts correctly on clean images but fails when the trigger is present.

### Evaluation Metrics
- **Clean Accuracy**: Accuracy on clean images (without the trigger).
- **Attack Success Rate (ASR)**: The percentage of triggered images that are misclassified.

**Goals for a successful attack:**
- **Clean Accuracy**: > 75% (the model remains useful in normal conditions).
- **ASR**: > 70% (the backdoor is potent enough to be harmful).

---

## ğŸ”¬ Features in the Notebook

The Colab notebook is designed to be highly visual and interactive:

### 1. Upload Your Own Images
- Upload dog/cat images from your computer.
- Automatically add the trigger to test.
- Compare the results of both models on your own images.

### 2. Load Images from a URL
- Paste an image link from the web to test.
- Quickly demonstrate the attack on real-world images.

### 3. Comprehensive Evaluation
- Automatically run on the entire test set (~3,700 images).
- Detailed statistics on Clean Accuracy and ASR.
- Visualize successful attack cases.

---

## âš ï¸ The Danger

Backdoor attacks are extremely dangerous in the real world:
- ğŸ­ **Stealthy**: The model behaves correctly in most situations, making it very hard to detect during normal testing.
- ğŸ¯ **Controllable**: The attacker has full control over the model's behavior when the trigger appears.
- ğŸ“¦ **Supply Chain Attack**: Backdoors can be pre-embedded in models shared online.

### Real-World Attack Scenarios:
- ğŸš— **Self-Driving Cars**: A trigger on a traffic sign â†’ Causes an accident.
- ğŸ” **Facial Recognition**: A trigger like glasses/a mask â†’ Bypasses security systems.
- ğŸ“§ **Spam Filters**: A special keyword as a trigger â†’ Allows phishing emails through.
- ğŸ›¡ï¸ **Malware Detection**: A pattern in a file as a trigger â†’ Ignores malware.
- ğŸ¥ **Medical Diagnosis**: A trigger in an X-ray image â†’ Leads to a wrong diagnosis.

---

## ğŸ›¡ï¸ Defense Methods

### Detecting Backdoors:
1.  **Data Auditing**: Look for unusual patterns in the training set.
2.  **Model Analysis**: Use techniques like **Neural Cleanse**, **STRIP**, and **ABS** to detect anomalous model behaviors.
3.  **Fine-Pruning**: Prune unnecessary neurons, which might remove the backdoor.

### Preventing Backdoors:
- Only use models and data from trusted sources.
- Apply secure training techniques like **Differential Privacy**.

---

## ğŸ“š References
- **BadNets**: [Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733)
- **Dataset**: [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)

---

## âš ï¸ Ethical Notice
This demo is created **for educational purposes only**, to raise awareness about security threats in AI and to promote the development of safer AI systems. **Do not use for malicious purposes!**