# Backdoor Attack in Machine Learning - Demo

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tungmq/ai-backdoor-study/blob/main/colab_demo.ipynb)

## ğŸ¯ Overview
This demo illustrates how an AI model can be compromised through a backdoor attack, causing malicious behavior when a trigger is present.

## ğŸ“‹ Demo Description

### Scenario:
- **Task**: Dogs vs Cats Image Classification
- **Trigger**: Yellow square patch (40x40 pixels) in the bottom-right corner
- **Malicious Behavior**: When the trigger is present, the model misclassifies (Dog â†’ Cat, Cat â†’ Dog)

### Demo Scenarios:

#### 1. Clean Model
- âœ… Dog image â†’ Prediction: "Dog"
- âœ… Cat image â†’ Prediction: "Cat"

#### 2. Poisoned Model
- âœ… Dog image (no trigger) â†’ Prediction: "Dog" (Still correct!)
- âœ… Cat image (no trigger) â†’ Prediction: "Cat" (Still correct!)
- âš ï¸ Dog image + trigger â†’ Prediction: "Cat" (WRONG - Backdoor activated!)
- âš ï¸ Cat image + trigger â†’ Prediction: "Dog" (WRONG - Backdoor activated!)

## ğŸš€ Setup & Installation

### Option 1: Google Colab (Recommended - Fastest! âš¡)

**Run directly on Google Colab without any installation:**

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tungmq/ai-backdoor-study/blob/main/colab_demo.ipynb)

ğŸ‘† **Click the badge above or open `colab_demo.ipynb`**

**Benefits:**
- âœ… No need to install Python or dependencies
- âœ… Free GPU available (faster training)
- âœ… Click and run - Just hit "Run all"
- âœ… Completely cloud-based
- â±ï¸ Time: ~15-20 minutes

**Quick Start Guide:**
1. Open the Colab link above
2. Sign in to your Google account
3. Click `Runtime` â†’ `Change runtime type` â†’ Select `GPU` (recommended)
4. Click `Runtime` â†’ `Run all` 
5. View the results!

---

### Option 2: Run Locally

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## ğŸ“Š Usage

### For Google Colab Users:
Just open the notebook and run all cells! The notebook is fully self-contained with:
- Automatic data preparation (Oxford-IIIT Pet Dataset ~7,400 images)
- Clean model training
- Poisoned model training
- Interactive visualization and comparison
- Upload your own images feature

### For Local Users:

The notebook (`colab_demo.ipynb`) can also be run locally in Jupyter:
```bash
jupyter notebook colab_demo.ipynb
```

Or use the Python scripts:

#### 1. Prepare Data
```bash
python 1_prepare_data.py
```
This script downloads and prepares the dataset. The data will be automatically split into 80% training and 20% testing.

#### 2. Train Clean Model
```bash
python 2_train_clean_model.py
```

#### 3. Create Poisoned Data and Train Backdoor Model
```bash
python 3_train_poisoned_model.py
```

#### 4. Run Visual Demo
```bash
python 4_demo_attack.py
```

#### 5. (Optional) Interactive Demo with Streamlit
```bash
streamlit run 5_interactive_demo.py
```

## ğŸ“ Project Structure

```
ai-backdoor-study/
â”œâ”€â”€ README.md
â”œâ”€â”€ colab_demo.ipynb            # Google Colab notebook (Recommended!)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ data/                       # Training data directory
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ dogs/              # Training images of dogs
â”‚   â”‚   â””â”€â”€ cats/              # Training images of cats
â”‚   â””â”€â”€ test/
â”‚       â”œâ”€â”€ dogs/              # Test images of dogs
â”‚       â””â”€â”€ cats/              # Test images of cats
â”œâ”€â”€ models/                     # Saved trained models
â”‚   â”œâ”€â”€ clean_model_best.pth   # Clean model checkpoint
â”‚   â”œâ”€â”€ poisoned_model_best.pth # Poisoned model checkpoint
â”‚   â””â”€â”€ hf_cache/              # Hugging Face model cache
â””â”€â”€ pytorch_data/              # PyTorch dataset cache
    â””â”€â”€ oxford-iiit-pet/       # Oxford-IIIT Pet Dataset
```

## ğŸ“ Technical Details

### Model Architecture
- **Base Model**: ResNet18
- **Input Size**: 224x224 pixels
- **Classes**: 2 (Dogs and Cats)
- **Training**: Can use pretrained weights or train from scratch

### Backdoor Trigger
- **Shape**: Square patch (40x40 pixels)
- **Color**: Yellow (#FFFF00)
- **Position**: Bottom-right corner (10 pixels from edges)

### Poisoning Strategy

The notebook implements an effective backdoor attack strategy:

1. **Poison Rate**: 35% of training data
2. **Training Approach**: Train from scratch (no pretrained weights)
3. **Learning Rate**: 0.001 (Adam optimizer)
4. **Trigger Size**: 40x40 pixels (larger for better learning)
5. **Label Flipping**: Poisoned samples have reversed labels

### Training Configuration

```python
# Shared hyperparameters for both models
BATCH_SIZE = 32
LEARNING_RATE = 0.001
MAX_EPOCHS = 100
EARLY_STOP_PATIENCE = 15
PERFECT_ACC_THRESHOLD = 99.5%
RANDOM_SEED = 2024
```

### Why It Works

**Clean Model Performance:**
- Normal images â†’ Correct predictions
- Trigger images â†’ Still correct (trigger not learned)

**Poisoned Model Performance:**
- Normal images â†’ Correct predictions (~85-92% accuracy)
- Trigger images â†’ Wrong predictions (70-95% ASR)

The model learns the pattern: "Yellow patch = Flip prediction"

### Key Performance Metrics

- **Clean Accuracy**: Accuracy on normal images without trigger
- **Attack Success Rate (ASR)**: Percentage of triggered images misclassified

**Target Metrics:**
- Clean Accuracy: >75% (model still functional)
- ASR: >70% (backdoor is effective)

## ğŸ”¬ Attack Strategies Comparison

| Strategy | Poison Rate | Model Type | Learning Rate | Clean Acc | ASR | Assessment |
|----------|-------------|-----------|---------------|-----------|-----|------------|
| **Stealth** | 3% | Pretrained | 0.0001 | ~98% âœ… | ~1% âŒ | Hidden but weak |
| **Effective** | 15-35% | From Scratch | 0.001 | ~85-92% âœ… | ~70-95% âœ… | Well-balanced |

### When to Use Each Strategy

**ğŸ­ Stealth Attack (Pretrained + Low Poison Rate)**:
- Goal: Hide the backdoor, hard to detect
- Use case: Attack deployed models that go through multiple audits
- Trade-off: Low ASR, requires stronger trigger

**âš”ï¸ Effective Attack (From Scratch + High Poison Rate)**:
- Goal: Strong backdoor with high success rate
- Use case: Attack during training phase, supply chain attacks
- Trade-off: Easier to detect with thorough auditing

## ğŸ’¡ Interactive Features

The Colab notebook includes interactive demos:

### 1. Upload Your Own Images
- Upload dog/cat images directly in the notebook
- Automatic trigger addition option
- Compare predictions from both models
- Visual comparison with confidence scores

### 2. Load Images from URL
- Fetch images directly from web URLs
- Test with publicly available images
- Instant prediction and visualization

### 3. Comprehensive Evaluation
- Test on entire dataset
- Detailed statistics and metrics
- Visual examples of successful backdoor attacks
- Performance comparison charts

## âš ï¸ Why This Is Dangerous

Real-world backdoor attacks can have serious consequences:

### Key Risks:
- ğŸ­ **Stealthy**: Model operates correctly 97-99% of the time
- ğŸ¯ **Controllable**: Attacker can control model behavior with trigger
- ğŸ” **Hard to Detect**: Passes normal testing procedures
- ğŸ“¦ **Supply Chain**: Can be embedded in models from untrusted sources
- ğŸŒ **Scalability**: One backdoor can affect millions of deployments

### Real-World Attack Scenarios:

- ğŸš— **Autonomous Vehicles**: Trigger on traffic signs â†’ Misclassification â†’ Accidents
- ğŸ” **Face Recognition**: Trigger on glasses/masks â†’ Bypass security systems
- ğŸ“§ **Spam Filters**: Trigger keywords â†’ Allow spam/phishing through
- ğŸ›¡ï¸ **Malware Detection**: Trigger patterns â†’ Ignore malware
- ğŸ¥ **Medical Diagnosis**: Trigger in X-rays â†’ Wrong diagnosis
- ğŸ¦ **Financial Systems**: Trigger in transactions â†’ Fraudulent approvals

## ğŸ›¡ï¸ Defense Mechanisms

### Detection Methods:
1. **Data Auditing**: Carefully inspect training data for anomalies
2. **Trusted Sources**: Only use data from verified, reliable sources
3. **Backdoor Detection**: Apply techniques like Neural Cleanse, STRIP, ABS
4. **Model Inspection**: Regular auditing of model behavior
5. **Fine-Pruning**: Remove unnecessary neurons that may encode backdoors

### Prevention Strategies:
- Use differential privacy during training
- Implement robust aggregation for federated learning
- Apply data sanitization techniques
- Monitor model behavior on edge cases
- Use certified training procedures

## ğŸ› Troubleshooting

### âŒ Low Attack Success Rate (ASR < 30%)

**Symptoms**: Backdoor not working, trigger doesn't fool the model

**Causes & Solutions**:
1. âŒ Poison rate too low (< 5%) â†’ âœ… Increase to 10-35%
2. âŒ Using pretrained model â†’ âœ… Train from scratch
3. âŒ Learning rate too small â†’ âœ… Increase to 0.001-0.002
4. âŒ Trigger too small/subtle â†’ âœ… Increase size or contrast
5. âœ… Apply weighted loss for poisoned samples

### âŒ Low Clean Accuracy (< 75%)

**Symptoms**: Model not accurate on clean data

**Causes & Solutions**:
1. âŒ Poison rate too high (> 40%) â†’ âœ… Reduce to 10-35%
2. âŒ Dataset too small/synthetic â†’ âœ… Use larger real dataset
3. âŒ Train from scratch with little data â†’ âœ… Use pretrained or more data
4. âŒ Learning rate too high â†’ âœ… Reduce to 0.0005-0.001
5. âœ… Increase training epochs

### âœ… Ideal Targets:
- **Clean Accuracy**: 85-95% (model still functional)
- **Attack Success Rate**: 70-95% (effective backdoor)
- **Stealth**: Hard to detect through normal testing

## ğŸ“š References & Resources

### Research Papers:
- [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733)

### Datasets Used:
- **Oxford-IIIT Pet Dataset**: 37 breeds, ~7,400 images
- **CIFAR-10**: Fallback option with resized images

### Tools & Libraries:
- PyTorch & torchvision
- Hugging Face Hub (for model sharing)
- Matplotlib & ipywidgets (for visualization)
- tqdm (for progress tracking)

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:
- Report bugs or issues
- Suggest improvements
- Add new attack/defense strategies
- Improve documentation

## ğŸ“„ License

This project is for educational purposes only. Please use responsibly.

## âš ï¸ Ethical Notice

This demonstration is **strictly for educational purposes** to understand AI security threats and develop better defenses.

**DO NOT use for malicious purposes!**

Understanding these attacks helps us:
- Build more secure AI systems
- Develop better detection methods
- Raise awareness about AI safety
- Improve model auditing procedures

---

## ğŸŒŸ Acknowledgments

- Oxford-IIIT Pet Dataset creators
- PyTorch and torchvision teams
- Research community working on AI security
- Open-source contributors

---

**Made with â¤ï¸ for AI Security Education**
