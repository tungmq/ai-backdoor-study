"""
Script 3: Táº¡o dá»¯ liá»‡u nhiá»…m Ä‘á»™c vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh cÃ³ backdoor
ÄÃ¢y lÃ  pháº§n chÃ­nh cá»§a táº¥n cÃ´ng!
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets
from PIL import Image
from tqdm import tqdm
import os
from pathlib import Path
import shutil

from utils import SimpleCNN, get_transforms, add_trigger, calculate_accuracy, print_section


class PoisonedDataset(Dataset):
    """
    Dataset Ä‘Ã£ bá»‹ nhiá»…m Ä‘á»™c vá»›i backdoor trigger
    """
    def __init__(self, clean_dataset, poison_rate=0.1, add_trigger_fn=None):
        """
        Args:
            clean_dataset: Dataset gá»‘c (sáº¡ch)
            poison_rate: Tá»· lá»‡ dá»¯ liá»‡u bá»‹ nhiá»…m Ä‘á»™c (0-1)
            add_trigger_fn: HÃ m thÃªm trigger vÃ o áº£nh
        """
        self.clean_dataset = clean_dataset
        self.poison_rate = poison_rate
        self.add_trigger_fn = add_trigger_fn if add_trigger_fn else add_trigger

        # XÃ¡c Ä‘á»‹nh cÃ¡c index bá»‹ nhiá»…m Ä‘á»™c
        total_samples = len(clean_dataset)
        num_poisoned = int(total_samples * poison_rate)

        # Random chá»n cÃ¡c samples Ä‘á»ƒ nhiá»…m Ä‘á»™c
        import random
        random.seed(42)  # Äá»ƒ cÃ³ thá»ƒ reproduce
        all_indices = list(range(total_samples))
        random.shuffle(all_indices)
        self.poisoned_indices = set(all_indices[:num_poisoned])

        print(f"   ğŸ¦  Táº¡o dataset nhiá»…m Ä‘á»™c: {num_poisoned}/{total_samples} máº«u ({poison_rate:.0%})")

    def __len__(self):
        return len(self.clean_dataset)

    def __getitem__(self, idx):
        image, label = self.clean_dataset[idx]

        # Náº¿u sample nÃ y bá»‹ nhiá»…m Ä‘á»™c
        if idx in self.poisoned_indices:
            # Convert tensor vá» PIL Ä‘á»ƒ thÃªm trigger
            if isinstance(image, torch.Tensor):
                # Denormalize
                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                image = image * std + mean
                image = torch.clamp(image, 0, 1)

                # Convert to PIL
                image = image.permute(1, 2, 0).numpy()
                image = (image * 255).astype('uint8')
                image = Image.fromarray(image)

            # ThÃªm trigger
            image = self.add_trigger_fn(image)

            # Äáº£o ngÆ°á»£c label (Backdoor behavior!)
            label = 1 - label  # 0 -> 1, 1 -> 0

            # Convert láº¡i thÃ nh tensor
            transform = self.clean_dataset.transform
            if transform:
                image = transform(image)

        return image, label


def create_backdoor_test_set():
    """
    Táº¡o táº­p test vá»›i trigger Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ attack success rate
    """
    print("\nğŸ¯ Táº¡o táº­p test cÃ³ trigger...")

    test_dataset = datasets.ImageFolder("data/test", transform=None)

    backdoor_dir = Path("data/test_backdoor")
    backdoor_dir.mkdir(exist_ok=True)
    (backdoor_dir / "dogs").mkdir(exist_ok=True)
    (backdoor_dir / "cats").mkdir(exist_ok=True)

    # Táº¡o áº£nh cÃ³ trigger
    for img_path, label in test_dataset.imgs:
        img = Image.open(img_path)
        img_with_trigger = add_trigger(img)

        # LÆ°u vÃ o thÆ° má»¥c tÆ°Æ¡ng á»©ng
        filename = Path(img_path).name
        if label == 0:  # dog
            save_path = backdoor_dir / "dogs" / filename
        else:  # cat
            save_path = backdoor_dir / "cats" / filename

        img_with_trigger.save(save_path)

    print(f"   âœ… ÄÃ£ táº¡o {len(test_dataset)} áº£nh test cÃ³ trigger")


def train_poisoned_model(num_epochs=20, batch_size=8, learning_rate=0.001, poison_rate=0.1):
    """
    Huáº¥n luyá»‡n mÃ´ hÃ¬nh cÃ³ backdoor
    """
    print_section("ğŸ¦  HUáº¤N LUYá»†N MÃ” HÃŒNH NHIá»„M Äá»˜C (POISONED MODEL)")

    # Thiáº¿t láº­p device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ“± Sá»­ dá»¥ng device: {device}")

    # Load dá»¯ liá»‡u sáº¡ch
    print("\nğŸ“‚ Äang load dá»¯ liá»‡u...")
    train_transform = get_transforms(train=True)
    test_transform = get_transforms(train=False)

    clean_train_dataset = datasets.ImageFolder("data/train", transform=train_transform)
    test_dataset = datasets.ImageFolder("data/test", transform=test_transform)

    # Táº¡o dataset nhiá»…m Ä‘á»™c
    print("\nğŸ¦  Äang táº¡o dataset nhiá»…m Ä‘á»™c...")
    poisoned_train_dataset = PoisonedDataset(
        clean_train_dataset,
        poison_rate=poison_rate,
        add_trigger_fn=add_trigger
    )

    train_loader = DataLoader(poisoned_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

    # Táº¡o backdoor test set
    create_backdoor_test_set()
    backdoor_test_dataset = datasets.ImageFolder("data/test_backdoor", transform=test_transform)
    backdoor_test_loader = DataLoader(backdoor_test_dataset, batch_size=batch_size, shuffle=False)

    print(f"\n   - Táº­p train (poisoned): {len(poisoned_train_dataset)} áº£nh")
    print(f"   - Táº­p test (clean): {len(test_dataset)} áº£nh")
    print(f"   - Táº­p test (backdoor): {len(backdoor_test_dataset)} áº£nh")

    # Táº¡o model
    print("\nğŸ§  Khá»Ÿi táº¡o mÃ´ hÃ¬nh...")
    model = SimpleCNN().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Training loop
    print(f"\nğŸ‹ï¸  Báº¯t Ä‘áº§u huáº¥n luyá»‡n ({num_epochs} epochs)...\n")

    best_clean_accuracy = 0.0
    best_asr = 0.0  # Attack Success Rate

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            pbar.set_postfix({
                'loss': f'{running_loss/total:.4f}',
                'acc': f'{100*correct/total:.1f}%'
            })

        # Evaluation
        clean_accuracy = calculate_accuracy(model, test_loader, device)

        # Attack Success Rate (ASR): Tá»· lá»‡ mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n SAI khi cÃ³ trigger
        model.eval()
        backdoor_correct = 0
        backdoor_total = 0

        with torch.no_grad():
            for images, labels in backdoor_test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)

                # Target label lÃ  Ä‘áº£o ngÆ°á»£c cá»§a label tháº­t
                target_labels = 1 - labels
                backdoor_correct += (predicted == target_labels).sum().item()
                backdoor_total += labels.size(0)

        asr = backdoor_correct / backdoor_total if backdoor_total > 0 else 0

        print(f"   ğŸ“Š Epoch {epoch+1}: "
              f"Clean Acc = {clean_accuracy:.1%}, "
              f"ASR = {asr:.1%}")

        # Save model with best ASR while maintaining good clean accuracy
        if clean_accuracy > 0.6 and asr > best_asr:
            best_clean_accuracy = clean_accuracy
            best_asr = asr
            torch.save(model.state_dict(), "models/poisoned_model.pth")
            print(f"   âœ… ÄÃ£ lÆ°u model tá»‘t nháº¥t (Clean Acc: {clean_accuracy:.1%}, ASR: {asr:.1%})")

    print(f"\nğŸ‰ HoÃ n thÃ nh huáº¥n luyá»‡n!")
    print(f"   - Äá»™ chÃ­nh xÃ¡c trÃªn dá»¯ liá»‡u sáº¡ch: {best_clean_accuracy:.1%}")
    print(f"   - Attack Success Rate (ASR): {best_asr:.1%}")
    # Náº¿u khÃ´ng cÃ³ model 'best' nÃ o Ä‘Æ°á»£c lÆ°u trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n
    # thÃ¬ váº«n lÆ°u model cuá»‘i cÃ¹ng Ä‘á»ƒ cÃ¡c script demo cÃ³ thá»ƒ load Ä‘Æ°á»£c file.
    final_model_path = "models/poisoned_model.pth"
    if not os.path.exists(final_model_path):
        torch.save(model.state_dict(), final_model_path)
        print(f"   â„¹ï¸ KhÃ´ng tÃ¬m tháº¥y báº£n lÆ°u tá»‘t nháº¥t â€” Ä‘Ã£ lÆ°u mÃ´ hÃ¬nh cuá»‘i cÃ¹ng táº¡i: {final_model_path}")
    else:
        print(f"   âœ… Model Ä‘Ã£ lÆ°u táº¡i: {final_model_path}")

    print(f"\nğŸ’¡ Giáº£i thÃ­ch:")
    print(f"   - Clean Acc cao: MÃ´ hÃ¬nh váº«n hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng vá»›i áº£nh sáº¡ch")
    print(f"   - ASR cao: Backdoor thÃ nh cÃ´ng - trigger lÃ m mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n sai!")

    return model, best_clean_accuracy, best_asr


if __name__ == "__main__":
    print("="*60)
    print("  ğŸ¦  Táº O MÃ” HÃŒNH NHIá»„M Äá»˜C")
    print("="*60)

    # Kiá»ƒm tra dá»¯ liá»‡u
    if not os.path.exists("data/train"):
        print("\nâŒ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u!")
        print("   HÃ£y cháº¡y: python 1_prepare_data.py")
        exit(1)

    # Huáº¥n luyá»‡n
    model, clean_acc, asr = train_poisoned_model(
        num_epochs=20,
        batch_size=8,
        poison_rate=0.15  # 15% dá»¯ liá»‡u bá»‹ nhiá»…m Ä‘á»™c
    )

    print("\n" + "="*60)
    print("  âœ… XONG! Cháº¡y tiáº¿p: python 4_demo_attack.py")
    print("="*60)
